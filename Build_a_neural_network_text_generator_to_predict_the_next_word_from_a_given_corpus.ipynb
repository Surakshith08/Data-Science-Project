{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 10**"
      ],
      "metadata": {
        "id": "cdL96BIHblLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:** To build a simple text generation model using a neural network that can predict the next word in a sequence based on a given corpus of text.\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "* Tokenization: The text corpus is converted into a sequence of numerical tokens, where each unique word is assigned a unique integer ID.\n",
        "* N-grams: The training data is created using n-grams, which are contiguous sequences of n items (in this case, words). For example, in the sentence \"jack likes apples\", the n-grams would be \"jack likes\" (2-gram) and \"jack likes apples\" (3-gram). The model is trained to predict the last word of an n-gram given the preceding words.\n",
        "* Padding: Since the n-grams have different lengths, padding is used to ensure that all input sequences have the same length.\n",
        "* Embedding Layer: This layer converts the integer token IDs into dense vectors (embeddings) that capture semantic relationships between words.\n",
        "* Flatten Layer: This layer flattens the output of the embedding layer into a 1D vector for feeding into the dense layers.\n",
        "Dense Layers: These are fully connected layers that learn complex patterns in the data.\n",
        "* Softmax Activation: The final dense layer uses a softmax activation function to output a probability distribution over the vocabulary, indicating the likelihood of each word being the next word in the sequence.\n",
        "* Categorical Crossentropy Loss: This loss function is used to measure the difference between the predicted probability distribution and the actual next word (represented as a one-hot encoded vector).\n",
        "* Adam Optimizer: This optimizer is used to update the model's weights during training to minimize the loss function."
      ],
      "metadata": {
        "id": "0BpsWDWEbpN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0hIw42IJg2R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sample corpus\n",
        "corpus = [\n",
        "\"jack likes apples\",\n",
        "\"jill likes oranges\",\n",
        "\"jack eats food\",\n",
        "\"jill eats fruits\",\n",
        "\"apples are tasty\",\n",
        "\"oranges are sweet\"\n",
        "]"
      ],
      "metadata": {
        "id": "w4uykBE4W6PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Tokenize text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1 # vocab size\n",
        "\n",
        "print(\"Vocabulary:\", tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk5is--IW-sU",
        "outputId": "8ac0528d-d26a-4f80-d2f6-abaec7cd7658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'jack': 1, 'likes': 2, 'apples': 3, 'jill': 4, 'oranges': 5, 'eats': 6, 'are': 7, 'food': 8, 'fruits': 9, 'tasty': 10, 'sweet': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Generate training sequences\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  print(\"Token List\",token_list)\n",
        "for i in range(1, len(token_list)):\n",
        "  n_gram_seq = token_list[:i+1]\n",
        "input_sequences.append(n_gram_seq)\n",
        "print(\"Inout Seq\",input_sequences)\n",
        "# Pad sequences\n",
        "max_seq_len = max(len(x) for x in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFpbQupPXArb",
        "outputId": "575e391c-6465-4892-d1c5-45b85a4be17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token List [1, 2, 3]\n",
            "Token List [4, 2, 5]\n",
            "Token List [1, 6, 8]\n",
            "Token List [4, 6, 9]\n",
            "Token List [3, 7, 10]\n",
            "Token List [5, 7, 11]\n",
            "Inout Seq [[5, 7, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "print(X,y)\n",
        "# Convert labels to one-hot\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(\"Training shape:\", X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfkhm75tXTX2",
        "outputId": "4b3af8da-3ea0-40b2-a4c8-a5420f9eb6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5 7]] [11]\n",
            "Training shape: (1, 2) (1, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Build model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n",
        "model.add(Flatten()) # feedforward style\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "tLWMUQrNXb7D",
        "outputId": "d7aae4a6-55c3-452e-9950-b2b96fe131f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train\n",
        "model.fit(X, y, epochs=20, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noh9ElpTXjU-",
        "outputId": "5df60c1e-f49c-475d-fc63-28e0939a1b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 2.4816\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0000e+00 - loss: 2.4721\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0000e+00 - loss: 2.4630\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.0000e+00 - loss: 2.4539\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.0000e+00 - loss: 2.4457\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 2.4376\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 2.4297\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 2.4217\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 2.4136\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 2.4054\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 2.3971\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 2.3892\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 2.3812\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 2.3730\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 2.3646\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 2.3560\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 2.3471\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 2.3380\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.3286\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 2.3189\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7878c6385250>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Generate text\n",
        "def predict_next_word(seed_text, next_words=3):\n",
        "  for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "    print(\"Predicted index \",predicted)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      print('word ', word,'index ',index)\n",
        "      if index == predicted:\n",
        "        seed_text += \" \" + word\n",
        "        break\n",
        "  return seed_text"
      ],
      "metadata": {
        "id": "mBwpKfVTXmB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next_word(\"jack likes\"))\n",
        "print(predict_next_word(\"jill\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "digVmAfrX9gK",
        "outputId": "41649339-5e1f-4848-a36e-683792f3d394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted index  [11]\n",
            "word  jack index  1\n",
            "word  likes index  2\n",
            "word  apples index  3\n",
            "word  jill index  4\n",
            "word  oranges index  5\n",
            "word  eats index  6\n",
            "word  are index  7\n",
            "word  food index  8\n",
            "word  fruits index  9\n",
            "word  tasty index  10\n",
            "word  sweet index  11\n",
            "Predicted index  [11]\n",
            "word  jack index  1\n",
            "word  likes index  2\n",
            "word  apples index  3\n",
            "word  jill index  4\n",
            "word  oranges index  5\n",
            "word  eats index  6\n",
            "word  are index  7\n",
            "word  food index  8\n",
            "word  fruits index  9\n",
            "word  tasty index  10\n",
            "word  sweet index  11\n",
            "Predicted index  [11]\n",
            "word  jack index  1\n",
            "word  likes index  2\n",
            "word  apples index  3\n",
            "word  jill index  4\n",
            "word  oranges index  5\n",
            "word  eats index  6\n",
            "word  are index  7\n",
            "word  food index  8\n",
            "word  fruits index  9\n",
            "word  tasty index  10\n",
            "word  sweet index  11\n",
            "jack likes sweet sweet sweet\n",
            "Predicted index  [11]\n",
            "word  jack index  1\n",
            "word  likes index  2\n",
            "word  apples index  3\n",
            "word  jill index  4\n",
            "word  oranges index  5\n",
            "word  eats index  6\n",
            "word  are index  7\n",
            "word  food index  8\n",
            "word  fruits index  9\n",
            "word  tasty index  10\n",
            "word  sweet index  11\n",
            "Predicted index  [11]\n",
            "word  jack index  1\n",
            "word  likes index  2\n",
            "word  apples index  3\n",
            "word  jill index  4\n",
            "word  oranges index  5\n",
            "word  eats index  6\n",
            "word  are index  7\n",
            "word  food index  8\n",
            "word  fruits index  9\n",
            "word  tasty index  10\n",
            "word  sweet index  11\n",
            "Predicted index  [11]\n",
            "word  jack index  1\n",
            "word  likes index  2\n",
            "word  apples index  3\n",
            "word  jill index  4\n",
            "word  oranges index  5\n",
            "word  eats index  6\n",
            "word  are index  7\n",
            "word  food index  8\n",
            "word  fruits index  9\n",
            "word  tasty index  10\n",
            "word  sweet index  11\n",
            "jill sweet sweet sweet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "* The training data (X, y) consists of only one sample ([[5, 7]], [11]) because of the way the input_sequences were generated in cell mFpbQupPXArb.\n",
        "* The model achieved 100% accuracy during training after a few epochs. This is likely due to the small dataset size and the simplicity of the task.\n",
        "* When predicting the next word for \"jack likes\" and \"jill\", the model consistently predicts \"sweet\". This is because the word \"sweet\" (token ID 11) is the only word that appears as the last word in the generated training sequence [[5, 7, 11]]."
      ],
      "metadata": {
        "id": "VIGIya2McdOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:** The model learned to predict the next word on the tiny dataset but is not practical for general text generation. A larger corpus and corrected sequence generation are needed for a useful model."
      ],
      "metadata": {
        "id": "qcM4XIwTclgn"
      }
    }
  ]
}