{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 5: Implement Q-learning for a 4×4 GridWorld where the agent starts at the top-left corner (0,0) and the goal is at the bottom-right corner (3,3). Use an ε-greedy policy for exploration, update the Q-table over multiple episodes, and display the learned optimal policy as a grid of arrows with the goal marked as G.**"
      ],
      "metadata": {
        "id": "bGlStqokzcKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procedure:\n",
        "1. Define the environment as a 4x4 grid with states and actions (up, down, left, right).\n",
        "2. Initialize a Q-table with zeros for all state-action pairs.\n",
        "3. Set Q-Learning parameters:\n",
        "4. Learning rate (α),\n",
        "5. Discount factor (γ),\n",
        "6. Exploration rate (ε),\n",
        "7. Number of episodes.\n",
        "8. Start from the initial state and apply the epsilon-greedy policy to choose actions.\n",
        "9. Transition to the next state, collect reward (1 at the goal, 0 otherwise).\n",
        "10. Update the Q-values using the Q-Learning update rule.\n",
        "11. Repeat the process for multiple episodes until the agent converges to the optimal policy.\n",
        "12. Analyze the learned Q-values to determine the best path to the goal.\n"
      ],
      "metadata": {
        "id": "vBk9f8cMzq2Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixCshX7nmIkN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Grid size\n",
        "n_rows, n_cols = 4,4\n",
        "n_states = n_rows*n_cols\n",
        "n_actions = 4 #up, down, left, right"
      ],
      "metadata": {
        "id": "20UltSUFmZ5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q-table initialization\n",
        "Q = np.zeros((n_states, n_actions))"
      ],
      "metadata": {
        "id": "5XIn4Qkomw0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "alpha = 0.1 #Learning rate\n",
        "gamma = 0.9 #Discount factor\n",
        "epsilon = 0.2 #Exploration rate\n",
        "episodes = 500"
      ],
      "metadata": {
        "id": "I-eZ2Njjm8zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Action mapping: 0=up, 1=down, 2=left, 3=right\n",
        "action_map = [(-1,0),(1,0),(0,-1),(0,1)]"
      ],
      "metadata": {
        "id": "s6b1QqExnA-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def state_to_index(row, col):\n",
        "  return row*n_cols + col\n",
        "\n",
        "def index_to_state(index):\n",
        "  return divmod(index, n_cols)\n",
        "\n",
        "def is_terminal(state):\n",
        "  return state == (3,3)"
      ],
      "metadata": {
        "id": "-COu03-Cn-nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "for ep in range(episodes):\n",
        "  row, col =0,0 # Start state\n",
        "  while not is_terminal((row,col)):\n",
        "    state_idx = state_to_index(row,col)\n",
        "    #Epsilon-greedy action selection\n",
        "    if random.uniform(0,1) < epsilon:\n",
        "      action = random.randint(0,3)\n",
        "    else:\n",
        "      action = np.argmax(Q[state_idx])\n",
        "    # Next state\n",
        "    d_row, d_col = action_map[action]\n",
        "    next_row = min(max(row+d_row, 0), n_rows-1)\n",
        "    next_col = min(max(col+d_col, 0), n_cols-1)\n",
        "    next_state_idx = state_to_index(next_row, next_col)\n",
        "    #Reward\n",
        "    reward = 1 if is_terminal((next_row, next_col)) else 0\n",
        "    # Q-learning update\n",
        "    Q[state_idx, action] =Q[state_idx, action] + alpha * (reward + gamma * np.max(Q[next_state_idx]) - Q[state_idx, action])\n",
        "      # Move to next state\n",
        "    row, col = next_row, next_col"
      ],
      "metadata": {
        "id": "IX1M7gVro80Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show learned policy\n",
        "directions = ['↑', '↓', '←', '→']\n",
        "policy = []\n",
        "for s in range(n_states):\n",
        "  row, col = index_to_state(s)\n",
        "  if is_terminal((row,col)):\n",
        "    policy.append('T')\n",
        "  else:\n",
        "    action = np.argmax(Q[s])\n",
        "    policy.append(directions[action])"
      ],
      "metadata": {
        "id": "dPDTudX6q-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display policy\n",
        "for i in range(n_rows):\n",
        "  print(policy[i*n_cols:(i+1)*n_cols])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNivRH4av0Hj",
        "outputId": "100fc169-2270-4b27-85b3-3f553c26642e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['→', '→', '→', '↓']\n",
            "['↑', '→', '↑', '↓']\n",
            "['↑', '↑', '↑', '↓']\n",
            "['↑', '↑', '↑', 'T']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg9-RauxxV5A",
        "outputId": "aca202aa-0d93-4f26-f579-ca9a18b0079b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49201478, 0.38082955, 0.51000289, 0.59049   ],\n",
              "       [0.49501654, 0.34556443, 0.5014202 , 0.6561    ],\n",
              "       [0.6202932 , 0.52759547, 0.56381745, 0.729     ],\n",
              "       [0.71013169, 0.81      , 0.62697507, 0.72012899],\n",
              "       [0.50804763, 0.00768908, 0.08985511, 0.05562385],\n",
              "       [0.05904899, 0.        , 0.09865821, 0.53871502],\n",
              "       [0.65607661, 0.12250694, 0.16654828, 0.27845624],\n",
              "       [0.67894872, 0.9       , 0.55955454, 0.757393  ],\n",
              "       [0.09711749, 0.        , 0.        , 0.        ],\n",
              "       [0.01370322, 0.        , 0.        , 0.        ],\n",
              "       [0.55422089, 0.        , 0.        , 0.        ],\n",
              "       [0.77323475, 1.        , 0.39050539, 0.87481726],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.0491152 , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noTY1KQKzNf8",
        "outputId": "33567ae6-a707-4c16-964e-187433a8ed55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9999999999999996)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "*   The agent starts with no prior knowledge, and Q-values are initialized to zero.\n",
        "*   Using epsilon-greedy action selection, the agent balances exploration of random moves with exploitation of learned Q-values.\n",
        "*   Over several episodes, the Q-values for optimal state-action pairs gradually increase.\n",
        "*   The agent learns to move step by step toward the terminal goal state (3,3) in the grid.\n",
        "*   The Q-table evolves into a representation of the best actions for each state.\n",
        "*   The practical illustrates how Q-Learning allows an agent to learn an optimal policy through\n",
        "repeated interactions with the environment.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_p210qcnzxyy"
      }
    }
  ]
}